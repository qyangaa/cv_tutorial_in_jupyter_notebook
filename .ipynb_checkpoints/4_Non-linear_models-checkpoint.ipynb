{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "## Problem with linear model\n",
    "+ Only captures linear relationship between feature and label\n",
    "+ Any combination of linear functions give linear function.\n",
    "\n",
    "## Simple approaches to introduce nonlinearity\n",
    "+ Use high-order function, e.g. $y = w_1x_1^5+w_2x_1^4+..$ \n",
    "+ -- Problem: problem: is that there are too many variables \n",
    "+ Use multiple linear classification models (combine binary to multi-class). It is different from combining linear functions because functions are not linearly combined. e.g. binary classification tree. need as many as binary classifier as number of class. \n",
    "\n",
    "### Linear Model: use a single weight matrix to linearly fit\n",
    "$$\n",
    "W\\left\\{\\begin{array}{ccc}\n",
    "w_{1} x_{1,1}+w_{2} x_{2,2}+ & \\cdots & w_{6} x_{1,6}=0 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "w_{1} x_{10, 1}+\\cdots & & w_{6} x_{10,6}=9\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "### Combine multiple binary classifier [Perceptron]\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "w_{1} x_{1,1}+w_{2} x_{1,2}+..=0 \\\\\n",
    "w_{1} x_{21}+w_{2} x_{2}+... = -1 \\\\\n",
    "w_{1} x_{10,1}+.. = -1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "$$\n",
    "XW = [1,-1,-1,...]\n",
    "$$\n",
    "\n",
    "Combine multiple binary classifier implementation\n",
    "+ keep model the same\n",
    "+ Challenge: get result from output of the model, when output is not strictly ~0 or ~1. See get_result(), determine which output is closest to the label, this logic is complicated to implement. It will be simplified in __logistic regression__. \n",
    "\n",
    "```\n",
    "for i in range(len(classes)):\n",
    "    y = model(feature,weights)\n",
    "    \n",
    "def get_result(y):\n",
    "    troch.argmin(torch.from_numpy(np.numpy([torch.min((torch.abs(y-i))) for i in range(0,10)])))\n",
    "```\n",
    "\n",
    "### Logistic regression, perceptron with sigmoid:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\text { sigmoid }\\left(X W_{1}\\right)=[1,0,0,0, \\cdots] \\\\\n",
    "\\text { sigmoid }\\left(X W_{2}\\right)=[0,1,0,0,0, \\cdots] \\\\\n",
    "\\text { sigmoid }\\left(X W_{3}\\right)=[0,0,1,0, \\ldots] \\\\\n",
    "\\vdots \\\\\n",
    "\\text { sigmoid }\\left(X W_{10}\\right)=[0,0, \\ldots,0,1]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "+ sigmoid(output) in model\n",
    "+ change loss to incorporate loss_weights, gives high weight for currently training class, to make sure each class gets trained.\n",
    "\n",
    "```\n",
    "def label2ground_truth(image_label):\n",
    "    gt = torch.ones(10,10)\n",
    "    gt = gt*0.0\n",
    "    loss_weights=torch.ones(10,10)\n",
    "    loss_weights = loss_weights*0.1\n",
    "    for label in image_label:\n",
    "        gt[label,label]=1.0 \n",
    "        loss_weights[label,label]=0.9\n",
    "\n",
    "def model(..):\n",
    "    ..\n",
    "    y= torch.sigmoid(h)\n",
    "    return y\n",
    "    \n",
    "def train_model(..):\n",
    "    gt,loss_weights=label2ground_truth(image_label)\n",
    "    loss = torch.sum((y-gt[i:i+1,:]).mul(y-gt[i:i+1,:]).mul(loss_weights[i:i+1,:]))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
